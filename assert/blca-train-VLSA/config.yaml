arch: VLSA
batch_size: 1
bp_every_batch: 32
ckpt_for_eval: last
cuda_id: 1
data_mode: patch
data_split_path: ./data_split/5foldcv/tcga_blca/splits_1.csv
data_split_seed: 1
dataset_name: tcga_blca
epochs: 10
es: false
es_patience: 20
es_start_epoch: 0
es_verbose: true
es_warmup: 0
eval_training_loader_per_epoch: false
evaluator: VL-IF
feat_format: pt
init_wt: false
loss_survemd_p: 2
loss_survemd_weight: 1.0
loss_survifmle_weight: 1.0
loss_type: SurvIFMLE-SurvEMD
lrs: false
lrs_factor: 0.5
lrs_patience: 10
model_saver_module_filter: prompt_encoder
monitor_metrics: loss
net_output_converter: softmax
num_shot: -1
num_workers: 4
opt_lr: 0.0002
opt_name: adam
opt_weight_decay: 1.0e-05
path_clip_model: /NAS02/Others/pretrained-models
path_cluster: null
path_coord: /NAS01/ExpData/tcga_blca/tiles-20x-s448/patches
path_graph: null
path_patch: /NAS01/ExpData/tcga_blca/tiles-20x-s448/feats-CONCH-vl-proj/pt_files
path_table: ./data_split/5foldcv/tcga_blca/mahmoodlab_tcga_blca_survival.csv
save_path: ./assert/blca-train-VLSA
save_prediction: true
seed: 42
task: vlsa
time_bins: 12
time_format: interval
vlsa_api: CONCH
vlsa_frozen_logit_scale: false
vlsa_img_encoder_dim_hid: 256
vlsa_img_encoder_dim_in: 512
vlsa_img_encoder_dim_reduction: 4
vlsa_img_encoder_drop_rate: 0.25
vlsa_img_encoder_frozen: false
vlsa_img_encoder_gated_query: false
vlsa_img_encoder_keep_ratio: 0.8
vlsa_img_encoder_name: VLFAN
vlsa_img_encoder_num_query: 12
vlsa_img_encoder_pred_head: default
vlsa_img_encoder_query: Text
vlsa_img_encoder_query_pooling: mean
vlsa_img_encoder_query_text_dim_reduction: 4
vlsa_img_encoder_query_text_keep_ratio: 0.8
vlsa_img_encoder_query_text_load_idx: tcga_blca_0
vlsa_img_encoder_query_text_load_path: tools/survival_text_prototypes.json
vlsa_img_encoder_query_text_method: TaskRes
vlsa_img_encoder_query_text_res_ratio: 0.5
vlsa_img_encoder_use_feat_proj: false
vlsa_pmt_learner_adapter_dim_reduction: 4
vlsa_pmt_learner_adapter_init_prompt_context_idx: 0
vlsa_pmt_learner_adapter_init_prompt_path: tools/survival_template_prompts.json
vlsa_pmt_learner_adapter_init_prompt_rank_idx: 0
vlsa_pmt_learner_adapter_keep_ratio: 0.8
vlsa_pmt_learner_adapter_method: default
vlsa_pmt_learner_adapter_num_ranks: 12
vlsa_pmt_learner_adapter_res_ratio: 0.5
vlsa_pmt_learner_coop_ckpt: null
vlsa_pmt_learner_coop_frozen_context_embeds: false
vlsa_pmt_learner_coop_frozen_rank_embeds: false
vlsa_pmt_learner_coop_init_prompt_context_idx: 0
vlsa_pmt_learner_coop_init_prompt_path: tools/survival_prompts.json
vlsa_pmt_learner_coop_init_prompt_rank_idx: 0
vlsa_pmt_learner_coop_method: rank
vlsa_pmt_learner_coop_num_base_ranks: 4
vlsa_pmt_learner_coop_num_context_tokens: 8
vlsa_pmt_learner_coop_num_ranks: 12
vlsa_pmt_learner_coop_num_tokens_per_rank: 4
vlsa_pmt_learner_coop_rank_specific_context: false
vlsa_pmt_learner_coop_rank_tokens_position: tail
vlsa_pmt_learner_name: CoOp
vlsa_pmt_learner_pretrained: false
vlsa_txt_encoder_frozen: true
vlsa_txt_encoder_name: mahmoodlab/conch
wandb_dir: /home/liup/repo/VLSA
wandb_prj: VLSA-IFMLE-Report
